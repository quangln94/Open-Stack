# Phân chia Compute trong Openstack

Khi nào cần tới các phương án phân chia tài nguyên?

Có 2 tình huống chính:
- Failure: Đảm bảo dịch vụ vẫn hoạt động bình thường khi xảy ra lỗi ở một hoặc một nhóm các phần cứng/phần mềm (tất nhiên không đến mức thảm họa như mất điện cả 1 tầng hay cả tổng trạm)
- Success: Lượng tài nguyên yêu cầu tăng nhiều, phát sinh do với thiết kế, cần mở rộng theo chiều ngang hoặc dọc.

Lựa chọn tài nguyên phần cứng tùy theo ứng dụng và dự phòng cho dịch vụ để đối phó với các lỗi chưa được dự tính trước hoặc theo khả năng giới hạn của của hypervisor. Người vận hành phải lên kế hoạch phân tách tài nguyên cho việc đảm bảo live time và phù hợp với từng loại dịch vụ cũng như kết hợp nhiều loại phần cứng trên mạng để sử dụng hạ tầng một cách hiệu quả nhất.

Khi số lượng thiết bị trong cloud tăng nhanh và nhiều, cần xác định chiến lược để đảm bảo độ trễ thấp, tránh cao tải API service (hoặc các thành phần khác thuộc khối quản lý). Openstack Nova cung cấp một số khái niệm cho phép phân nhóm tài nguyên. Mỗi chiến lược cách ly (Segregation Strategy) cũng đều có điểm mạnh và điểm thiếu xót. Chúng ta cần hiểu để xây dựng thiết kế hạ tầng cloud phù hợp.

## 1. Availability Zones

Mỗi AZ là một nhóm các compute nodes, được chia dựa theo fault domains, ví dụ: đặt trên chung một Rack. Tất cả các node đều kết nối chung ToR switch và chung PDU, hình thành một fault domain theo tài nguyên hạ tầng. Ý tưởng của AZ ánh xạ với khái niệm hardware failure domain. Trong trường hợp này có thể là Rack bị mất điện hoặc ToR switch lỗi sẽ dẫn đến cả Rack bị ảnh hưởng. Để giải quyết vấn đề này, khi thiết lập ứng dụng, 2 module cùng chức năng cần được đặt tối thiểu trên 2 AZ khác nhau để đảm bảo dự phòng.

<img src=https://i.imgur.com/kJs1nO4.png>

## 2. Host Aggregates

Host Aggregates(HG) là chiến thuật nhóm các compute node cung cấp tài nguyên theo các đặc tính riêng. Ví dụ nhóm các compute kết nối với cụm storage 100% SSD là một HG, nhóm compute có kết nối với storage có performance thấp hơn (SAS 10k) là 1 cụm. Tương tự với hạ tầng cho NFV thì nhóm các compute có sử dụng SR-IOV, Hugepages thành các HG khác nhau.

<img src=https://i.imgur.com/aCKt7Qb.png>

Ví dụ Host Aggregates, 3 HG: Storage – optimized, Network Optimized, High-freg-cpu. Trong trường hợp này, 1 host có thể thuộc 2 hoặc nhiều HG

Sau khi nhóm như trên, tương ứng với yêu cầu từng ứng dụng, loại dịch vụ sẽ đặt vào các HG tương ứng để đảm bảo chất lượng dịch vụ. ví dụ như DB thì cần đặt vào HG có storage đọc ghi nhanh...

Trên thao tác thực tế, trước hết cần thiết lập Metadata cho các host có resource tương ứng, sau đó tạo HG. Tiếp theo là tạo ra các flavor (VM template) với metadata tương ứng.

## 3. Nova Cells

Thông thường khi cài đặt OpenStack, tất cả compute node cần gửi thông tin vào message queue và DB server (sử dụng nova-conductor). Cách làm này khá nặng nề cho messages queue và DB. Khi cloud của bạn mở rộng, có rất nhiều compute server kết nối cùng hạ tầng tài nguyên và gây nghẽn.

Khái niệm Cell ra đời để giúp mở rộng tài nguyên tính toán. Nova Cell là cách để mở rộng compute workload bằng cách phần phối tải lên hạ tầng tài nguyên như DB, message queue, tới nhiều instance.

Kiến trúc Nova cell tạo ra các group compute nodes, sắp xếp theo mô hình cây, được gọi là cell. Mỗi cell có DB và message queue của nó. Chiến thuật này là để ràng buộc kết nối đến DB, message queue theo các cell khác nhau.

Bằng cách nào kiến trúc này hoạt động? 
Hãy nhìn vào các thành phần và cách nó giao tiếp với nhau. Như đã trình bày bên trên, các cell được sắp xếp theo kiểu cây, gốc của cây là API cell và nó chạy Nova API nhưng không chạy Nova compute service, trong khi các node khác được gọi là compute cell, chạy tất cả Nova service.

Cell’s architecture làm việc bằng cách tách Nova API service (thành phần nhận đầu vào từ toàn bộ cách thành phần khác của Nova compute). Tương tác giữa Nova API và các thành phần khác được thay thế bởi Message queue based RPC call, khi Nova API nhận một call để khởi động instance mới, nó sử dụng Cell RPC call để sắp xếp instance này trên 1 compute cell đang sẵn sàng. Compute cell chạy DB, message queue của nó và hoàn thành tập nova service ngoại trừ Nova API. Compute cell sau đó chạy instance bằng cách sắp xếp nó trên một compute node:

<img src=https://i.imgur.com/HS4oHBh.png>

Mặc dù Cell đã được triển khai trong Nova trước đó, họ không thấy nó được triển khai rộng dãi và vẫn được đánh dấu là thử nghiệm. Hiện tại, Cell vẫn là một tính năng tùy chọn, nhưng Nova project đang làm việc với các triển khai tiếp theo của kiến trúc Cell với tầm nhìn đây là kiến trúc mặc định khi triển khai compute cloud. Trong kiến trúc cell hiện tại, sắp xếp một instance yêu cầu 2 level schedule, đầu tiên là lựa chọn cell, sau đó là lựa chọn compute. Hi vọng trong các cải tiến tới (v2 API) sẽ loại bỏ phương án sắp lịch 2 level.

## 4. Regions

Khái niệm về Cell cho phép mở rộng compute cloud bằng cách cách ly các compute node thành các group nhưng vẫn đảm bảo một Nova Api duy nhất. Nova Regions cho phép một cách tiếp cận khác, nhiều Nova API endpoint được sử dụng để chạy các VM. Mỗi Nova region có một cài đặt đầy đủ của Nova, với tập các compute node và Nova API endpoint của nó. Các Nova Region khác nhau của một OpenStack cloud sử dụng chung Keystone service cho xác thực và quảng bá các Nova API endpoints. End user sẽ có khả năng lựa chọn Region nơi chúng muốn VM được chạy.

<img src=https://i.imgur.com/qAnydPR.png>

## 5. Workload segegation sử dụng Affinity policy

Trong phần trước, chúng ta đã tìm hiểu phân chia tài nguyên theo AZ, HG. Từ đó hiểu VM sẽ được lựa chọn và chạy trên host nào. Cách này chỉ cho phép xử lý mỗi lần một VM.

Trong trường hợp bạn có 2 VM chạy 2 module ứng dụng HA cho nhau, với các phương án bên trên có thể chúng sẽ được chọn đặt chung vào cùng 1 compute. Khi compute này down sẽ làm dịch vụ chết.

=> Thực hiện cách ly workload dựa trên affinity policy. Xem ví dụ sau.

- Sử dụng Nova client tạo 2 policy: affinity và anti-affinity:

nova server-group svr-grp1 affinity

nova server-group svr-grp2 anti-affinity

- Affinity: cho phép các VM đặt cùng host, trong khi anti-affinity bắt buộc các VM phải đặt trên các compute node khác nhau

- Lệnh sau sẽ chạy 2 VM thuộc group 1 và 2 vm này sẽ luôn được đặt cùng compute node.
```sh
nova boot --image image1 --hint group=svr-grp1-uuid --flavor "Standard 1" vm1
nova boot --image image1 --hint group=svr-grp1-uuid --flavor "Standard 1" vm2
```

<img src=https://i.imgur.com/PNJh0jy.png>

<img src=https://i.imgur.com/rCeOG9E.png>

## Tài liệu tham khảo
- https://cloudfun.vn/threads/hieu-cac-cach-phan-chia-compute-trong-openstack.134/
